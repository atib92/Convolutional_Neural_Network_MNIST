{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST DIGIT CLASSIFICATION USING A CONVOLUTIONAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matibudd\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'my_mnist_cnn.ipynb',\n",
       " 't10k-images-idx3-ubyte.gz',\n",
       " 't10k-labels-idx1-ubyte.gz',\n",
       " 'train-images-idx3-ubyte.gz',\n",
       " 'train-labels-idx1-ubyte.gz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir()\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "f_train_images = gzip.open('train-images-idx3-ubyte.gz','r')\n",
    "f_test_images  = gzip.open('t10k-images-idx3-ubyte.gz','r')\n",
    "f_train_labels = gzip.open('train-labels-idx1-ubyte.gz','r')\n",
    "f_test_labels  = gzip.open('t10k-labels-idx1-ubyte.gz','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRAINING SET LABEL FILE (train-labels-idx1-ubyte):\\n[offset] [type]          [value]          [description] \\n0000     32 bit integer  0x00000801(2049) magic number (MSB first) \\n0004     32 bit integer  60000            number of items \\n0008     unsigned byte   ??               label \\n0009     unsigned byte   ??               label \\n........ \\nxxxx     unsigned byte   ??               label\\nThe labels values are 0 to 9.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''TRAINING SET IMAGE FILE (train-images-idx3-ubyte):\n",
    "[offset] [type]          [value]          [description] \n",
    "0000     32 bit integer  0x00000803(2051) magic number \n",
    "0004     32 bit integer  60000            number of images \n",
    "0008     32 bit integer  28               number of rows \n",
    "0012     32 bit integer  28               number of columns \n",
    "0016     unsigned byte   ??               pixel \n",
    "0017     unsigned byte   ??               pixel \n",
    "........ \n",
    "xxxx     unsigned byte   ??               pixel'''\n",
    "\n",
    "'''TRAINING SET LABEL FILE (train-labels-idx1-ubyte):\n",
    "[offset] [type]          [value]          [description] \n",
    "0000     32 bit integer  0x00000801(2049) magic number (MSB first) \n",
    "0004     32 bit integer  60000            number of items \n",
    "0008     unsigned byte   ??               label \n",
    "0009     unsigned byte   ??               label \n",
    "........ \n",
    "xxxx     unsigned byte   ??               label\n",
    "The labels values are 0 to 9.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size        = 28\n",
    "\n",
    "#lets work with only 600 training and 100 test data to get the algo up and runinig\n",
    "\n",
    "num_train_images  = 60000 \n",
    "num_test_images   = 10000\n",
    "\n",
    "# READ FIRST 16 BYTES TO REACH START OF PIXEL DATA\n",
    "\n",
    "f_train_images.read(16)\n",
    "f_test_images.read(16)\n",
    "\n",
    "# READ FIRST 8 BYTES TO REACH START OF LABEL DATA\n",
    "\n",
    "f_train_labels.read(8)\n",
    "f_test_labels.read(8)\n",
    "\n",
    "# STORE DATA IN BYTE FORMAT \n",
    "\n",
    "buf_train = f_train_images.read(image_size * image_size * num_train_images)\n",
    "buf_test  = f_test_images.read(image_size * image_size * num_test_images)\n",
    "\n",
    "buf_train_label = f_train_labels.read(num_train_images)\n",
    "buf_test_label = f_test_labels.read(num_test_images)\n",
    "\n",
    "# CONVERT DATA FROM BYTE FORMAT TO NUMPY ARRAY SO THAT WE CAN EASILY USE RESHAPE AND OTHER NUMPY METHODS\n",
    "\n",
    "data_train_images = np.frombuffer(buf_train, dtype=np.uint8).astype(np.float32)\n",
    "data_test_images  = np.frombuffer(buf_test, dtype=np.uint8).astype(np.float32)\n",
    "\n",
    "data_train_label  = np.frombuffer(buf_train_label, dtype=np.uint8).astype(np.float32)\n",
    "data_test_label   = np.frombuffer(buf_test_label, dtype=np.uint8).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images  = data_train_images.reshape([num_train_images,image_size*image_size])\n",
    "test_images   = data_test_images.reshape([num_test_images,image_size*image_size])\n",
    "train_labels  = data_train_label.reshape([num_train_images,-1])\n",
    "test_labels   = data_test_label.reshape([num_test_images,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60000, 784), (10000, 784), (60000, 1), (10000, 1)]\n"
     ]
    }
   ],
   "source": [
    "# WE CAN NOW THINK OF IMAGE DATA AS A NX28X28 , LABELS AS NX1 ARRAY \n",
    "print([np.shape(train_images), np.shape(test_images), np.shape(train_labels), np.shape(test_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19f9543b5f8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19f953c3978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "plt.imshow(train_images[num_train_images-1].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_images   NX784\\ntrain_labels   NX1\\ntest_images    MX784\\ntest_labels    MX1\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUT DATA \n",
    "'''\n",
    "train_images   NX784\n",
    "train_labels   NX1\n",
    "test_images    MX784\n",
    "test_labels    MX1\n",
    "'''\n",
    "# Convert labels to NX10 and MX10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_label(label,classes):\n",
    "    label_mat = np.zeros([len(label),classes])\n",
    "    for i in range(len(label)):\n",
    "        label_mat[i][label[i][0]] = 1.0\n",
    "    return label_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = create_mnist_label(train_labels.astype(int),10)\n",
    "test_labels = create_mnist_label(test_labels.astype(int),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights\n",
    "def weight(shape):\n",
    "    return tf.Variable(initial_value=tf.truncated_normal(shape=shape,mean=0.0,stddev=0.01,dtype=tf.float32))\n",
    "# define bias\n",
    "def bias(shape):\n",
    "    return tf.Variable(tf.constant(0.1,dtype=tf.float32,shape=shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a 2-D convolution of an imput image with a filter\n",
    "# try to read the documentation for tf.nn.conv2d\n",
    "def conv2_D(in_image,in_filter):\n",
    "    #in_image  => [batch X H X W X Channels]\n",
    "    #in_filter => [h X w X in_channel X out_channel]\n",
    "    return tf.nn.conv2d(in_image, in_filter, strides=[1,1,1,1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a convolution nerural layer\n",
    "def convolution_layer(image, filter_shape):\n",
    "    #create filter of shape = filter_shape\n",
    "    in_filter = weight(filter_shape)\n",
    "    #one bias per in_channel like we have one bias per neuron in dense layer\n",
    "    in_bias   = bias([filter_shape[3]]) # input in shape format and not in int\n",
    "    return tf.nn.relu(conv2_D(image,in_filter)+in_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dense neural layer\n",
    "def dense_layer(flat_image, num_neurons):\n",
    "    #flat_image => [Batch X L]\n",
    "    in_weight = weight([int(flat_image.get_shape()[1]),num_neurons])\n",
    "    in_bias = bias([num_neurons])# input in shape format and not int\n",
    "    return tf.nn.relu(tf.matmul(flat_image,in_weight)+in_bias) # python '+' will take care of replicating b 'Batch' number of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pooling layer\n",
    "def pool_2(image):\n",
    "    return tf.nn.max_pool(image,ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_images   NX784\\ntrain_labels   NX10\\ntest_images    MX784\\ntest_labels    MX10\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUT DATA \n",
    "'''\n",
    "train_images   NX784\n",
    "train_labels   NX10\n",
    "test_images    MX784\n",
    "test_labels    MX10\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define next batch utiltity\n",
    "def get_next_batch(train_images,train_labels,itr,batch_size):\n",
    "    # first dimension of both images and the labels are the indices \n",
    "    return (train_images[itr*batch_size:(itr+1)*batch_size], train_labels[itr*batch_size:(itr+1)*batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input that will be fed will be batch_size X 784\n",
    "X = tf.placeholder(dtype=tf.float32,shape=[None,image_size*image_size])\n",
    "# The algo needs a reshaped image of batch_size X 28 X 28 X 1 (Use 'None' for placeholders and -1 for tensor)\n",
    "X_image = tf.reshape(X,shape=[-1,image_size,image_size,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(dtype=tf.float32,shape=[None,10]) # 10 Classification class for the 10 possible digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dropout layer\n",
    "def drop(x,hold_prob):\n",
    "    return tf.nn.dropout(x, keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1 : convolution with 5X5 filter  + 2X2 Poll\n",
    "X_1 = convolution_layer(X_image,[5,5,1,32])\n",
    "X_1_poll = pool_2(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2 : convolution with 5X5 filter + 2X2 Poll\n",
    "X_2 = convolution_layer(X_1_poll,[5,5,32,64])\n",
    "X_2_poll = pool_2(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the image\n",
    "X_3 = tf.reshape(X_2_poll,[-1,7*7*64]) # First dimesion should be batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 3 : Dense Network with flattened image\n",
    "X_3_1 = dense_layer(X_3,1024)\n",
    "X_3_drop = drop(X_3_1,hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 4 : Dense layer\n",
    "X_4 = dense_layer(X_3_drop,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-c0c5302186d2>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the loss metric\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "model = tf.train.AdamOptimizer(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = model.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initate the tensorflow session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps,batch_size = 5000,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on step 0\n",
      "Accuracy is:\n",
      "18.57\n",
      "Currently on step 100\n",
      "Accuracy is:\n",
      "91.67\n",
      "Currently on step 200\n",
      "Accuracy is:\n",
      "93.76\n",
      "Currently on step 300\n",
      "Accuracy is:\n",
      "96.69\n",
      "Currently on step 400\n",
      "Accuracy is:\n",
      "97.28\n",
      "Currently on step 500\n",
      "Accuracy is:\n",
      "97.69\n",
      "Currently on step 600\n",
      "Accuracy is:\n",
      "97.76\n",
      "Currently on step 700\n",
      "Accuracy is:\n",
      "98.05\n",
      "Currently on step 800\n",
      "Accuracy is:\n",
      "97.87\n",
      "Currently on step 900\n",
      "Accuracy is:\n",
      "98.06\n",
      "Currently on step 1000\n",
      "Accuracy is:\n",
      "98.11\n",
      "Currently on step 1100\n",
      "Accuracy is:\n",
      "98.53\n",
      "Currently on step 1200\n",
      "Accuracy is:\n",
      "97.8\n",
      "Currently on step 1300\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 1400\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 1500\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 1600\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 1700\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 1800\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 1900\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 2000\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 2100\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 2200\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 2300\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 2400\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 2500\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 2600\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 2700\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 2800\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 2900\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 3000\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 3100\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 3200\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 3300\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 3400\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 3500\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 3600\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 3700\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 3800\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 3900\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 4000\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 4100\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 4200\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 4300\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 4400\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 4500\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 4600\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 4700\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 4800\n",
      "Accuracy is:\n",
      "97.6\n",
      "Currently on step 4900\n",
      "Accuracy is:\n",
      "97.6\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for itr in range(steps):\n",
    "        X_batch,y_batch = get_next_batch(train_images,train_labels,itr,batch_size)\n",
    "        sess.run(train, feed_dict = {X:X_batch,y:y_batch,hold_prob:0.5})\n",
    "        if (itr % 100 == 0):\n",
    "            # tf.equal will return a boolean array [100010111100000....batch_size]\n",
    "            # tf.reduce_mean will find percentage of 1s = percentage of correct classifications\n",
    "            print('Currently on step {}'.format(itr))\n",
    "            print('Accuracy is:')\n",
    "            performance = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y,1),tf.argmax(y_pred,1)),tf.float64))\n",
    "            print(sess.run(100*performance, feed_dict = {X:test_images,y:test_labels,hold_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
